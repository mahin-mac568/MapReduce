On Scalable and Efficient Distributed Failure Detectors
Indranil Gupta

Tushar D. Chandra

Germán S. Goldszmidt

Department of Computer
Science
Cornell University
Ithaca, NY 14853, USA

IBM T.J. Watson Research
Center
P.O. Box 704
Yorktown Heights, NY, USA

IBM T.J. Watson Research
Center
P.O. Box 704
Yorktown Heights, NY, USA

gupta@cs.cornell.edu

tushar@us.ibm.com

gsg@us.ibm.com

ABSTRACT
Process groups in distributed applications and services rely
on failure detectors to detect process failures completely,
and as quickly, accurately, and scalably as possible, even
in the face of unreliable message deliveries. In this paper,
we look at quantifying the optimal scalability, in terms of
network load, (in messages per second, with messages having a size limit) of distributed, complete failure detectors
as a function of application-speciﬁed requirements. These
requirements are 1) quick failure detection by some nonfaulty process, and 2) accuracy of failure detection. We
assume a crash-recovery (non-Byzantine) failure model, and
a network model that is probabilistically unreliable (w.r.t.
message deliveries and process failures). First, we characterize, under certain independence assumptions, the optimum worst-case network load imposed by any failure detector that achieves an application’s requirements. We then
discuss why traditional heartbeating schemes are inherently
unscalable according to the optimal load. We also present
a randomized, distributed, failure detector algorithm that
imposes an equal expected load per group member. This
protocol satisﬁes the application deﬁned constraints of completeness and accuracy, and speed of detection on an average. It imposes a network load that diﬀers from the optimal by a sub-optimality factor that is much lower than that
for traditional distributed heartbeating schemes. Moreover,
this sub-optimality factor does not vary with group size (for
large groups).

Keywords
Distributed systems, Failure detectors, Eﬃciency, Accuracy,
Scalability.

1.

INTRODUCTION

Failure detectors are a central component in fault-tolerant
distributed systems based on process groups running over
unreliable, asynchronous networks eg., group membership
protocols [3], supercomputers, computer clusters [13], etc.

The ability of the failure detector to detect process failures
completely and eﬃciently, in the presence of unreliable messaging as well as arbitrary process crashes and recoveries,
can have a major impact on the performance of these systems. “Completeness” is the guarantee that the failure of
a group member is eventually detected by every non-faulty
group member. “Eﬃciency” means that failures are detected
quickly, as well as accurately (i.e., without too many mistakes).
The ﬁrst work to address these properties of failure detectors was by Chandra and Toueg [5]. The authors showed
why it is impossible for a failure detector algorithm to deterministically achieve both completeness and accuracy over
an asynchronous unreliable network. This result has lead to
a ﬂurry of theoretical research on other ways of classifying failure detectors, but more importantly, has served as
a guide to designers of failure detector algorithms for real
systems. For example, most distributed applications have
opted to circumvent the impossibility result by relying on
failure detector algorithms that guarantee completeness deterministically while achieving eﬃciency only probabilistically [1, 2, 4, 6, 7, 8, 14].
The recent emergence of applications for large scale distributed systems has created a need for failure detector algorithms that minimize the network load (in bytes per second,
or equivalently, messages per second with a limit on maximum message size) used, as well as the load imposed on
participating processes [7, 14]. Failure detectors for such
settings thus seek to achieve good scalability in addition to
eﬃciency, while still (deterministically) guaranteeing completeness.
Recently, Chen et al. [6] proposed a comprehensive set of
metrics to measure the Quality of Service (QoS) of complete
and eﬃcient failure detectors. This paper presented three
primary metrics to quantify the performance of a failure detector at one process detecting crash-recovery failures of a
single other process over an unreliable network. The authors
proposed failure detection time, and recurrence time and duration times of mistaken detection as the primary metrics for
complete and eﬃcient failure detectors. However, the paper
neither deal with the optimal relation among these metrics,
nor focussed on distributed or scalable failure detectors.
In this paper, we ﬁrst address the question of quantifying
the optimum worst-case network load (in messages per sec-

ond, with a limit on messages sizes) needed by a complete
distributed failure detector protocol to satisfy the eﬃciency
requirements as speciﬁed by the application. We are concerned with distributed failure detectors working in a group
of uniquely identiﬁable processes, which are subject to failures and recoveries, and communicate over an unreliable network. We deal with complete failure detectors that satisfy
application-deﬁned eﬃciency constraints of 1) (quickness)
detection of any group member failure by some non-faulty
member within a time bound, and 2) (accuracy) probability (within this time bound) of no other non-faulty member
detecting a given non-faulty member as having failed.
The ﬁrst (quickness) requirement merits further discussion.
Many systems, such as multi-domain server farm clusters [7,
13] and virtual synchrony implementations [3] rely on a single or a few central computers to aggregate failure detection
information from across the system. These computers are
then responsible for disseminating that information across
the entire system. In such systems, eﬃcient detection of
a failure depends on the time the failure is ﬁrst detected
by a non-faulty member. Even in the absence of a central
server, notiﬁcation of a failure is typically communicated,
by the ﬁrst member to detect it, to the entire group via a
(possibly unreliable) broadcast [3]. Thus, although achieving completeness is important, eﬃcient detection of a failure
is more often related with the time to the ﬁrst detection, by
another non-faulty member, of the failure.
We derive the optimal worst-case network load (in messages
per second, with a limit on maximum message size) imposed
on the network by a complete failure detector satisfying the
above application-deﬁned constraints. We then discuss why
the traditional and popular distributed heartbeating failure detection schemes (eg., [7, 14]) do not achieve these optimal scalability limits. Finally, we present a randomized
distributed failure detector that can be conﬁgured to meet
the application-deﬁned constraints of completeness and accuracy, and expected speed of detection. With reasonable
assumptions on the network unreliability (member and message failure rates of up to 15%), the worst-case network load
imposed by this protocol has a sub-optimality factor that is
much lower than that of traditional distributed heartbeat
schemes. This sub-optimality factor does not depend on
group size (in large groups), but only on the applicationspeciﬁed eﬃciency constraints and the network unreliability
probabilities. Furthermore, the average load imposed per
member is independent of the group size.
In arriving at these results, we will assume that message loss
and member failures can each be characterized by probabilistic distributions, independent across messages and failures.
While the practicality of these assumptions in real networks
will probably be subject to criticism, these assumptions are
necessary in order to take this ﬁrst step towards quantifying
and achieving scalable and eﬃcient failure detectors. Besides, we believe that these independence assumptions are
partially justiﬁed because of 1) the randomized nature of the
new failure detector algorithm, and 2) the large temporal
separation between protocol periods, typically O(seconds)
in practice (mitigating much of the correlation among message loss probability distributions).

The rest of the paper is organized as follows. Section 2
brieﬂy summarizes previous work in this area. In Section 3,
we formally describe the process group model assumed in
this paper. Section 4 presents a discussion of how an application can specify eﬃciency requirements to a failure detector, and quantiﬁes the optimal worst-case network load
a failure detector must impose, in order to meet these requirements. Section 5 presents the new randomized failure
detector protocol. We conclude in section 6.

2. PREVIOUS WORK
Chandra and Toueg [5] were the ﬁrst to formally address
the completeness and accuracy properties of failure detectors. Subsequent work has focused on diﬀerent properties
and classiﬁcations of failure detectors. This area of literature has treated failure detectors as oracles used to solve the
Distributed Consensus/Agreement problem [9], which is unsolvable in the general asynchronous network model. These
classiﬁcations of failure detectors are primarily based on the
weakness of the model required to implement them, in order to solve the Distributed Consensus/Agreement problem
[11].
Proposals for implementable failure detectors have sometimes assumed network models with weak unreliability semantics eg., timed-asynchronous model [8], quasi-synchronous
model [2], partial synchrony model [12], etc. These proposals have treated failure detectors only as a tool to eﬃciently
reach agreement, ignoring their eﬃciency from an application designer’s viewpoint. For example, most failure detectors such as [12] provide eventual guarantees, while applications are typically concerned about real timing constraints.
In most real-life distributed systems, the failure detection
service is implemented via variants of the “Heartbeat mechanism” [1, 2, 4, 6, 7, 8, 14], which have been popular as
they guarantee the completeness property. However, all existing heartbeat approaches have shortcomings. Centralized
heartbeat schemes create hot-spots that prevent them from
scaling. Distributed heartbeat schemes oﬀer diﬀerent levels
of accuracy and scalability depending on the exact heartbeat dissemination mechanism used, but we show that they
are inherently not as eﬃcient and scalable as claimed.
Probabilistic network models have been used to analyze heartbeat failure detectors in [4, 6], but only with a single process
detecting failures of a single other process. [6] was the ﬁrst
paper to propose metrics for non-distributed heartbeat failure detectors in the crash-recovery model. These metrics
were not inclusive of scalability concerns.
Our work diﬀers from all this prior work in that it is the
ﬁrst to approach the design of failure detectors from a distributed application developer’s viewpoint. We quantify the
performance of a failure detector protocol as the network
load it requires to impose on the network, in order to satisfy the application-deﬁned constraints of completeness, and
quick and accurate detection1 . We also present an eﬃcient
and scalable distributed failure detector. The new failure
detector incurs a constant expected load per process, thus
1
We will state these application-deﬁned requirements formally in Section 4.

avoiding the hot-spot problem of centralized heartbeating
schemes.

3.

MODEL

We consider a large group of n ( 1) members2 . This set of
potential group members is ﬁxed a priori. Group members
have unique identiﬁers. Each group member maintains a
list, called a view, containing the identities of all other group
members (faulty or otherwise). Our protocol speciﬁcation
and analysis assumes that this maximal group membership
is always the same at all members, but our results can be
extended to a model with dynamically changing membership
and members with incomplete views, using methodologies
similar to [10].
Members may suﬀer crash (non-Byzantine) failures, and recover subsequently. Unlike other papers on failure detectors
(eg., [14]) that consider a member as faulty if they are perturbed and sleep for a time greater than some pre-speciﬁed
duration, our notion of failure considers that a member is
faulty if and only if it has really crashed. Perturbations at
members that might lead to message losses are accounted for
in the message loss rate pml (which we will deﬁne shortly).
Whenever a member recovers from a failure, it does so into
a new incarnation that is distinguishable from all its earlier
incarnations. At each member, an integer in non-volatile
storage, that is incremented every time the member recovers, suﬃces to serve as the member’s incarnation number.
The members in our group model thus have crash-recovery
semantics with incarnation numbers distinguishing diﬀerent
failures and recoveries. When a member Mi crashes (fails),
it does so in its current incarnation (say its l’th incarnation).
We say that such a failure is “detected” at exactly the ﬁrst
instant of time that some other non-faulty member detects
either 1) failure of Mi in incarnation greater than or equal
to l, or 2) recovery of Mi in an incarnation strictly greater
than l.
We characterize the member failure probability by a parameter pf . pf is the probability that a random group member
is faulty at a random time. Member crashes are assumed to
be independent across members.
We assume no synchronization of clocks across group members. We only require that each individual member’s clock
drift rate (from some ﬁxed clock rate) remains constant.
Members communicate using unicast (point-to-point) messaging on an asynchronous, fault-prone network. Since we
are interested in characterizing the network bandwidth utilized, we will assume that maximal message sizes are a constant, containing at most a few bytes of data (assuming a
bound on the size of message identiﬁers and headers, as is
typical in IP packets).
Each message sent out on the network fails to be delivered
at its recipient (due to network congestion, buﬀer overﬂow
at the sender or receiver due to member perturbations, etc.)
with probability pml ∈ (0, 1). The worst-case message prop2
All of which are either processes, or servers, or network
adaptors etc.

agation delay (from sender to receiver through the network)
for any delivered message is assumed to be so small compared to the application-speciﬁed detection time (typically
O( several seconds )) that henceforth, for all practical purposes, we can assume that each message is either delivered
immediately at the recipient with probability (1 − pml ), or
never reaches the recipient.3
This message loss distribution is also assumed to be independent across messages. Message delivery losses could, in fact,
be correlated in such a network. However, if applicationspeciﬁed failure detection times are much larger than message propagation and congestion repair times in the network,
messages exchanged by the failure detector will have considerable temporal separation. This reduces the correlation
among the loss distributions of diﬀerent messages. Randomized selection of message destinations in the new failure
detector also weakens such message loss correlation.
In the rest of the paper, we use the shorthands qf and qml
instead of (1 − pf ) and (1 − pml ) respectively.

4. SCALABLE AND EFFICIENT FAILURE
DETECTORS
The ﬁrst formal characterization of the properties of failure
detectors was oﬀered in [5], which laid down the following
properties for distributed failure detectors in process groups:
• Strong/Weak Completeness: crash-failure of any
group member is detected by {all/some} non-faulty
members4 ,
• Strong Accuracy: no non-faulty group member 5 is
declared as failed by any other non-faulty group member.

[5] also showed that a perfect failure detector i.e., one which
satisﬁes both Strong Completeness and Strong Accuracy, is
suﬃcient to solve distributed Consensus, but is impossible
to implement in a fault-prone network.
Subsequent work on designing eﬃcient failure detectors has
attempted to trade oﬀ the Completeness and Accuracy properties in several ways. However, the completeness properties required by most distributed applications have lead to
the popular use of failure detectors that guarantee Strong
Completeness always, even if eventually [1, 2, 4, 5, 6, 7,
8, 14]. This of course means that such failure detectors
cannot guarantee Strong Accuracy always, but only with a
probability less than 1. For example, all-to-all (distributed)
3
This assumption is made for simplicity. In fact, the optimality results of section 4 hold if pml is assumed to be the
probability of message delivery within T time units after its
send. The randomized protocol of section 5 and its analysis
can be extended to hold if pml is the probability of message
delivery within a sixth of the protocol period.
4
Recollect that in our model, since members recover with
unique incarnations, detection of a member’s failure or recovery also implies detection of failure of all it’s previous
incarnations.
5
in its current incarnation

heartbeating schemes have been popular because they guarantee Strong Completeness (since a faulty member will stop
sending heartbeats), while providing varying degrees of accuracy.
We have explained in Section 1 why in many distributed
applications, although the failure of a group member must
eventually be known to all non-faulty members, it is important to have the failure detected quickly by some non-faulty
member (and not necessarily all non-faulty members). In
other words, the quickness of failure detectors depends on
the time from a member failure to Weak Completeness with
respect to that failure, although Strong Completeness is a
necessary property.
The requirements imposed by an application (or its designer)
on a failure detector protocol can thus be formally speciﬁed
and parameterized as follows:
1. Completeness: satisfy eventual Strong Completeness
for member failures.
2. Efficiency:

That brings us to the question - what is the optimal worstcase network load, call it L∗ , that is needed to satisfy the
above application-deﬁned requirements - Completeness,
Speed (T ), Accuracy (PM (T )) ? We are able to answer this question in the network model discussed earlier
when the group size n is very large ( 1), and PM (T ) is
very small (  pml ).

Theorem 1. Any distributed failure detector algorithm
for a group of size n ( 1) that deterministically satisﬁes the
Completeness, Speed, Accuracy requirements above, for
given values of T and PM (T ) ( pml ), imposes a minimal
worst-case network load (messages per time unit, as deﬁned
above) of:
L∗ = n ·

log(PM (T ))
log(pml ) · T

Furthermore, there is a failure detector that achieves this
minimal worst-case bound while satisfying the Completeness, Speed, Accuracy requirements.

(a) Speed: every member failure is detected by some
non-faulty group member within T time units after its occurrence (T  worst-case message round
trip time).

L∗ is thus the optimal worst-case network load required to
satisfy the Completeness, Speed, Accuracy requirements.

(b) Accuracy: at any time instant, for every nonfaulty member Mi not yet detected as failed, the
probability that no other non-faulty group member will (mistakenly) detect Mi as faulty within
the next T time units is at least (1 − PM (T )).

Proof. We prove the ﬁrst part of the theorem by showing
that each non-faulty group member could transmit up to
log(PM (T ))
messages in a time interval of length T .
log(pml )

T and PM (T ) are thus parameters speciﬁed by the application (or its designer). For example, an application designer
might specify T = 3 seconds, and PM (3 seconds) = 10−8 .
To measure the scalability of a failure detector algorithm, we
use the worst-case network load it imposes - this is denoted
as L. Since several messages may be transmitted simultaneously even from one group member, we deﬁne:

Deﬁnition 1. The worst-case network load L of a failure
detector protocol is the maximum number of messages transmitted by any run of the protocol within any time interval
of length T , divided by T .
We also require that the failure detector impose a uniform
expected send and receive load at each member due to this
traﬃc.
The goal of a near-optimal failure detector algorithm is thus
to satisfy the above requirements (Completeness, Efficiency) while guaranteeing:
• Scale: the worst-case network load L imposed by the
algorithm is close to the optimal possible, with equal
expected load per member.

Consider a group member Mi at a random point in time
t. Let Mi not be detected as failed yet by any other group
member, and stay non-faulty until at least time t + T . Let
m be the maximum number of messages sent by Mi , in the
time interval [t, t + T ], in any possible run of the failure
detector protocol starting from time t.
Now, at time t, the event that “all messages sent by Mi in
the time interval [t, t+T ] are lost” happens with probability
at least pm
ml . Occurrence of this event entails that it is indistinguishable to the set of the rest of the non-faulty group
members (i.e., members other than Mi ) as to whether Mi is
faulty or not. By the Speed requirement, this event would
then imply that Mi is detected as failed by some non-faulty
group member between t and t + T .
Thus, the probability that at time t, a given non-faulty member Mi that is not yet detected as faulty, is detected as failed
by some other non-faulty group member within the next T
time units, is at least pm
ml . By the Accuracy requirement,
(T ))
≤
PM
(T
),
which implies that m ≥ log(PM
.
we have pm
ml
log(pml )
A failure detector that satisﬁes the Completeness, Speed,
Accuracy requirements and meets the L∗ bound works as
follows. It uses a highly available, non-faulty server as a
(T ))
group leader6 . Every other group member sends [ log(PM
]
log(pml )
“I am alive” messages to this server every T time units. The
6

The set of central computers, that collect failure information and disseminate it to the system, can be designated as
the server.

server declares a member as failed when it does not receive
any “I am alive” message from it for T time units7 .

servers to disseminate failure information. These systems
require only some other non-faulty member to detect a given
failure.

Corollary: The optimal bound of Theorem 1 applies to the
crash-stop model as well.

Other heartbeating schemes, such as Centralized heartbeating (as discussed in the proof of Theorem 1) and heartbeating along a logical ring of group members [7], can be conﬁgured to meet the optimal load L∗ , but have problems such
as creating hot-spots (centralized heartbeating) or unpredictable failure detection times in the presence of multiple
simultaneous faults at larger group sizes (heartbeating in a
ring).

Proof: By exactly the same arguments as in the proof of
Theorem 1.
¾
Deﬁnition 2. The sub-optimality factor of a failure detector algorithm that imposes a worst-case network load
L, while satisfying the Completeness and Efficiency requirements, is deﬁned as LL∗ .
In the traditional distributed Heartbeating failure detection
algorithms, every group member periodically transmits a
“heartbeat” message (with an incremented counter) to every other group member. A member Mi is declared as failed
by a non-faulty member Mj when Mj does not receive heartbeats from Mi for some consecutive heartbeat periods (this
duration being the detection time T ).
Distributed heartbeating schemes have been the most popular implementation of failure detectors because they guarantee Completeness - a failed member will not send any
more heartbeat messages. However, the accuracy and scalability guarantees of heartbeating algorithms diﬀer, depending entirely on the actual mechanism used to disseminate
heartbeats.
In the simplest implementation, each member Mi transmits
a few “I am alive” messages to each group member it knows
of, every T time units. The worst-case number of messages
transmitted by each member per unit time is θ(n), and the
worst-case total network load L is θ(n2 ). The sub-optimality
factor (i.e., LL∗ ) varies as θ(n), for any values of pml , pf and
PM (T ).
The Gossip-style failure detection service, proposed by van
Renesse et al. [14], uses a mechanism where every tgossip
time units, each member gossips a θ(n) list of the latest
heartbeat counters (for all group members) to a few other
randomly selected group members. The authors show that
under this scheme, a new heartbeat count typically takes
an average time of θ[log(n) · tgossip ] to reach an arbitrary
other group member. The Speed requirement thus leads
T
]. The worst-case network
us to choose tgossip = θ[ log(n)
load imposed by the Gossip-style heartbeat scheme is thus
2
n2
] = θ[ n ·log(n)
]. The sub-optimality factor varies as
θ[ tgossip
T
θ[n · log(n)], for any values of pml , pf and PM (T ).

5. A RANDOMIZED DISTRIBUTED
FAILURE DETECTOR PROTOCOL
In the preceding sections, we have characterized the optimal worst-case load imposed by a distributed failure detector that satisﬁes the Completeness, Speed and Accuracy requirements, for application speciﬁed values of T and
PM (T ) (Theorem 1). We have then studied why traditional
heartbeating schemes are inherently not scalable.
In this section, we relax the Speed condition to detect a failure within an expected (rather than exact, as before) time
bound of T time units after the failure. We then present a
randomized distributed failure detector algorithm that guarantees Completeness with probability 1, detection of any
member failure within an expected time T from the failure,
and an Accuracy probability of (1 − PM (T )). The protocol imposes an equal expected load per group member, and
a worst-case (and average case) network load L that diﬀers
from the optimal L∗ of Theorem 1 by a sub-optimality factor
(i.e., LL∗ ) that is independent of group size n ( 1). In such
large groups, at reasonable values of member and message
delivery failure rates pf and pml , this sub-optimality factor
is much lower than the sub-optimality factors of the traditional distributed heartbeating schemes discussed in the
previous section.

5.1 New Failure Detector Algorithm
The failure detector algorithm uses two parameters: protocol period T  (in time units) and integer k, which is the size
of failure detection subgroups. We will show how the values
of these parameters can be conﬁgured from the required values of T and PM (T ), and the network parameters pf , pml .
Parameters T  and k are assumed to be known a priori at
all group members. Note that this does not need clocks
to be synchronized across members, but only requires each
member to have a steady clock rate to be able to measure
T .

In fact, distributed heartbeating schemes do not meet the
optimality bound of Theorem 1 because they inherently attempt to communicate a failure notiﬁcation to all group
members. As we have seen above, this is an overkill for
systems that can rely on a centralized coordinated set of

The algorithm is formally described in Figure 1. At each
non-faulty member Mi , steps (1-3) are executed once every
T  time units (which we call a protocol period), while steps
(4,5,6) are executed whenever necessary. The data contained
in each message is shown in parentheses after the message. If
sequence numbers are allowed to wrap around, the maximal
message size is bounded from above.

7
This implementation, which is essentially a centralized
heartbeat mechanism, is undesirable as it requires a highly
available server and has bad load balancing (does not satisfy
the Scale property).

Figure 2 illustrates the protocol steps initiated by a member
Mi , during one protocol period of length T  time units. At
the start of this protocol period at Mi , a random member

Integer pr; /* Local period number */

TIME

M

Every T  time units at Mi :
0.
1.

2.

3.

choose random M

pr := pr + 1
Select random member Mj from view
Send a ping(Mi , Mj , pr) message to Mj
Wait for the worst-case message round-trip time for
an ack(Mi , Mj , pr) message
If have not received an ack(Mi , Mj , pr) message yet
Select k members randomly from view
Send each of them a ping-req(Mi , Mj , pr) message
Wait for an ack(Mi , Mj , pr) message until
the end of period pr
If have not received an ack(Mi , Mj , pr) message yet
Declare Mj as failed

i
j

On receipt of a ping-req(Mm , Mj , pr) (Mj = Mi )
Send a ping(Mi , Mj , Mm , pr) message to Mj
On receipt of an ack(Mi , Mj , Mm , pr) message from Mj
Send an ack(Mm , Mj , pr) message to received to Mm

Anytime at Mi :
5.

On receipt of a ping(Mm , Mi , Ml , pr) message from
member Mm
Reply with an ack(Mm , Mi , Ml , pr) message to Mm

j

ping

ack
choose k random
members
T’

ping-req(Mj)

...

ping
ping
ack

Anytime at Mi :
4.

M

ack

Figure 2: Example protocol period at Mi . This
shows all the possible messages that a protocol period may initiate. Some message contents excluded
for simplicity.

Anytime at Mi :
6.

On receipt of a ping(Mm , Mi , pr) message from member Mm
Reply with an ack(Mm , Mi , pr) message to Mm

Figure 1: Protocol steps at a group member Mi .
Data in each message is shown in parentheses after
the message. Each message also contains the current
incarnation number of the sender.
is selected, in this case Mj , and a ping message sent to it.
If Mi does not receive a replying ack from Mj within some
time-out (determined by the message round-trip time, which
is  T ), it selects k members at random and sends to each
a ping-req message. Each of the non-faulty members among
these k which receives the ping-req message subsequently
pings Mj and forwards the ack received from Mj , if any, back
to Mi . In the example of Figure 2, one of the k members
manages to complete this cycle of events as Mj is up, and
Mi does not suspect Mj as faulty at the end of this protocol
period.
In the above protocol, member Mi uses a randomly selected
subgroup of k members to out-source ping-req messages,
rather than sending out k repeat ping messages to the target
Mj . The eﬀect of using the randomly selected subgroup is
to distribute the decision on failure detection across a subgroup of (k + 1) members. Although we do not analyze it
in this paper, it can be shown that the new protocol’s properties are preserved even in the presence of some degree of
variation of message delivery loss probabilities across group
members. Sending k repeat ping messages may not satisfy
this property. Our analysis in Section 5.2 shows that the
cost (in terms of sub-optimality factor of network load) of
using a (k + 1)-sized subgroup is not too signiﬁcant.

5.2 Analysis
In this section, we calculate, for the above protocol, the
expected detection time of a member failure, as well as
the probability of an inaccurate detection of a non-faulty

member by some other (at least one) non-faulty member.
This will lead to calculation of the values of T  and k, for
the above protocol, as a function of parameters specifying
application-speciﬁed requirements and network unreliability, i.e., T , PM (T ), pf , pml .
For any group member Mj , faulty or otherwise,
Pr
=


[at least one non-faulty member chooses to
ping Mj (directly) in a time interval T  ]
1
1 − (1 − · qf )n−1
n
1 − e−qf (since n  1)

Thus, the expected time between a failure of member Mj
and its detection by some non-faulty member is
E[T ] = T  ·

1
eqf

−qf = T · qf
e −1
1−e

(1)

This gives us a conﬁgurable value for T  as a function of
T , pf .
Now, denote
C(pf ) =

eqf
e −1
qf

At any given time instant, a non-faulty member Mj will be
detected as faulty by another non-faulty member Ml within
the next T time units if Ml chooses to ping Mj within the
next T time units and does not receive any acks, directly
or indirectly from transitive ping-req’s, from Mj . Then,
PM (T ), the probability of inaccurate failure detection of
member Mj within the next T time units, is simply the
probability that there is at least one such member Ml in the
group.
A random group member Ml is non-faulty with probability

qf , and the probability of such a member choosing to ping
Mj within a time interval T is n1 · C(pf ). Given this, the
probability that such a Ml receives back no acks, direct or
indirect, according to the protocol of section 5.1 equals
((1 −

2
qml
)

· (1 − qf ·

Then, from Theorem 1 and equations (1),(2),
log[
L
L∗





qf
2
4
· C(pf ) · (1 − qml
) · (1 − qf · qml
)k ]n−1
n
2

4

1 − e−qf ·(1−qml )·(1−qf ·qml )
(since n  1)

k

·C(pf )

k=

e

−1

)

4
log(1 − qf · qml
)

= g(pf , pml ) +

eqf
log(pml )
· qf
]
4
log(1 − qf · qml ) e − 1
(4b)

]
(2)

Proof. From the above discussion and equations (1),
(2).

Finally, we upper-bound the worst-case and expected network load (L, E[L] respectively) imposed by this failure detector protocol.
The worst-case network load occurs when, every T  time
units, each member initiates steps (1-6) in the algorithm
of Figure 1. Steps (1,6) involve at most 2 messages, while
steps (2-5) involve at most 4 messages per ping-req target
member. Therefore, the worst-case network load imposed
by this protocol (in messages/time unit) is
1
T

1
· f (pf , pml )
−log(PM (T ))
(4a)

Theorem 2. This randomized failure detector protocol:
(a) satisﬁes eventual Strong Completeness, i.e., the Completeness requirement,
(b) can be conﬁgured via equations (1) and (2) to meet the
requirements of (expected) Speed, and Accuracy, and
(c) has a uniform expected send/receive load at all group
members.

= n · [2 + 4 · k] ·

]

where g(pf , pml ) is:

Thus, the new randomized failure detector protocol can be
conﬁgured using equations (1) and (2) to satisfy the Speed
and Accuracy requirements with parameters E[T ], PM (T ).
Moreover, given a member Mj that has failed (and stays
failed), every other non-faulty member Mi will eventually
choose to ping Mj in some protocol period, and discover Mj
as having failed. Hence,

L

]

L thus diﬀers from the optimal L∗ by a factor that is independent of the group size n. Furthermore, (3) can be written
1
as:
as a linear function of −log(PM
(T ))

This gives us
PM (T )
q
2 )· e f
(qf ·(1−qml
qf

)

4
log(1 − qf · qml
)
eqf
log(pml )
×[ qf
·
]
e − 1 log(PM (T ))

[4 ·
log[

−1

(3)

L
L∗

2
4
) · (1 − qf · qml
)k · C(pf )
qf · (1 − qml
(since PM (T )  1)

e

= [2 + 4 ·

4
qml
)k )

Therefore,
PM (T ) = 1 − [1 −

PM (T )
q
2 )· e f
(qf ·(1−qml
qf

and f (pf , pml ) is:
[{2 − 4 ·

2
log(qf · (1 − qml
)·

log(1 − qf ·

q

e f
)
q
e f −1

4
qml
)

} × (−log(pml )) ·

eqf
]
e −1
qf

(4c)

Theorem 3. The sub-optimality factor LL∗ of the protocol
of Figure 1, is independent of group size n ( 1). Furthermore,
1. if f (pf , pml ) < 0,
(a) LL∗ is monotonically increasing with −log(PM (T )),
and
(b) As PM (T ) → 0+ , LL∗ → g(pf , pml )−
2. if f (pf , pml ) > 0,
(a) LL∗ is monotonically decreasing with −log(PM (T )),
and
(b) As PM (T ) → 0+ , LL∗ → g(pf , pml )+
Proof. From equations (4a) through (4c).
We next calculate the average network load imposed by the
new failure detector algorithm. Every T  time units, each
non-faulty member (numbering (n · qf ) on an average) executes steps (1-3), in the algorithm of Figure 1. Steps (1,6)
involve at most 2 messages, while steps (2-5) (which are executed only if no ack is received from the target of the ping
2
))
of step (1) - this happens with probability (1 − qf · qml
involve at most 4 messages per non-faulty ping-req target
member. Therefore, the average network load imposed by
this protocol (in messages/time unit) is
E[L]

2
≤ n · qf · [2 + (1 − qf · qml
) · 4 · k] ·

1
T

Then, from Theorem 1 and equations (1),(2),

E[L]
L∗

log[
≤

2
qf · [2 + 4 · (1 − qf · qml
)·

PM (T )

q
2 )· e f )
(qf ·(1−qml
q
e f −1

log(1 − qf ·
eqf
log(pml )
×[ qf
·
]
e − 1 log(PM (T ))

4
qml
)

]

ln(Pm(T))=-10
ln(Pm(T))=-30
g(pf,pml)

(L/L*)

]

(5)

28
24
20
16
12
8
4

Even E[L] can be upper-bounded from the optimal L∗ by a
factor that is independent of the group size n.
0
L
L∗

E[L]
L∗

and
go very high compared to
Do the values of
the ideal value of 1.0 ? The answer is a ’No’ when values of pf , pml are low, yet reasonable. Figure 3(a) shows
the variation of LL∗ as in equation (3), at low but reasonable values of pf , pml , and PM (T ). This plot shows that
the sub-optimality factor of the network load imposed by
the new failure detector rises as pml and pf increase, or
PM (T ) decreases, but is bounded above by the function
g(pf , pml ), at all values of PM (T ). This happens because
f (pf , pml ) < 0 at such low values of pf and pml , as seen
from Figure 3(b) - Theorem 3.1 thus applies here. From
ﬁgure 3(a), the function g(pf , pml ) (bottom-most surface),
does not attain too high values (staying below 26 for the
values shown). Thus the performance of the new failure detector algorithm is good for reasonable assumptions on the
network unreliability.
stays very
Figure 3(c) shows that the upper bound on E[L]
L∗
low (below 8) for values of pf and pml up to 15%. Moreover, as PM (T ) is decreased, the bound on E[L]
actually
L∗
decreases. This curve reveals the advantage of using randomization in the failure detector. Unlike traditional distributed heartbeating algorithms, the average case network
load behavior of the new protocol is much lower than the
worst-case network load behavior.
Figure 3 reveals that for values of pf and pml below 15%,
the LL∗ for the new randomized failure detector stays below 26, and E[L]
stays below 8. Further, as is evident from
L∗
equations (3) and (5), the variation of these sub-optimality
factors does not depend on the group size (at large group
sizes). Compare this with the sub-optimality factors of distributed heartbeating schemes discussed in Section 4, which
are typically at least θ[n].

0.03
0.06
pf 0.090.12
0.150

0.15
0.12
0.09
pml
0.06
0.03

(a) Variation of LL∗ (according to equation
(3)) versus pml , pf , at diﬀerent values of
PM (T ). For low values of pml and pf ,
g(pf , pml ) is an upper bound on LL∗ .

f(pf,pml) > 0 ?
1

0
0.8
0.6
0

0.2

0.4
0.4
pf

pml

0.2
0.6

0.8 0

(b) Values of pf , pml for which f (pf , pml ) is
positive or negative.
ln(Pm(T))=-10
ln(Pm(T))=-30
(E[L]/L*) <=
8
6
4

In reality, message loss rates and process failure rates could
vary from time to time. The parameters pf and pml , needed
to conﬁgure protocol parameters T  and k, may be diﬃcult to estimate. However, Figure 3 shows that assuming
reasonable bounds on these message loss rates/failure rates
and using these bounds to conﬁgure the failure detector sufﬁces. In other words, conﬁguring protocol parameters with
pf , pml = 15% will ensure that the failure detector preserves
the application speciﬁed constraints (T , PM (T )) while imposing a network load that diﬀers from the optimal worstcase load L∗ by a factor of at most 26 in the worst-case, and
8 in the average case, as long as the message loss/process
failure rates do not exceed 15% (this load is lower when loss
or failure rates are lower).

2
0

0

0.03
0.06
pf 0.090.12
0.150

0.15
0.12
0.09
pml
0.06
0.03

(c) Variation of E[L]
versus pml , pf
L∗
(according to equation (5)).
Figure 3: Performance of new failure detector algorithm

5.3 Future Work and Optimizations
At Cornell University, we are currently testing performance
of a scalable distributed membership service that uses the
new randomized failure detection algorithm.
Extending the above protocol to the crash-stop model inherent to dynamic groups involves several protocol extensions.
Every group member join, leave or failure detection entails a
broadcast to the non-faulty group members in order to update their view. Further, this broadcast may not be reliable.
Implementing this protocol over a group spanning several
subnets requires that the load on the connecting routers or
gateways be low. The protocol currently imposes an O(n)
load (in bytes per second) on such routers during every protocol period. Reducing this load inevitably leads to compromising some of the Efficiency properties of the protocol,
as pings are sent less frequently across subnets.
The protocol can also be optimized to trade oﬀ worse Scale
properties for better Accuracy properties. One such optimization is to follow a failure detection (by an individual non-faulty member through the described protocol) by
multicast of a suspicion of that failure, waiting for some
time before turning this suspicion into a declaration of a
member failure. With such a suspicion multicast in place,
protocol periods at diﬀerent non-faulty group members, targeting this suspected member, can be correlated to improve
the Accuracy properties. This would also reduce the eﬀect
of correlated message failures on the frequency of mistaken
failure declarations.
A disadvantage of the protocol is that since messages are restricted to contain at most a few bytes of data, large message
headers mean higher overheads per message. The protocol also precludes optimizations involving piggy-backed messages, primarily due to the random selection of ping targets.
The discussion in this paper also points us to several new
and interesting questions.
Is it possible to design a failure detector algorithm that,
for an asynchronous network setting, satisﬁes Completeness, Efficiency, Scale requirements, and the Speed requirement (section 4) with a deterministic bound on time to
detection of a failure (T ), rather than as an average case as
we have done in this paper ?8 Notice that this is not diﬃcult
to achieve in a synchronous network setting (by modifying
the new failure detector algorithm to choose ping targets
in a deterministic and globally known manner during every
protocol period).
We also leave as an open problem the speciﬁcation and realization of optimality load conditions for a failure detector with the Speed timing parameter T set as the time to
achieve Strong Completeness for any group member failure
(rather than just Weak Completeness).
8
Heartbeating along a logical ring among group members
(eg., [7]) seems to provide a solution to this question. However, as pointed out before, ring heartbeating has unpredictable failure detection times in the presence of multiple
simultaneous failures.

Of course, it would be ideal to extend all such results to
models that assume some degree of correlation among message losses, and perhaps even member failures.

6. CONCLUDING COMMENTS
In this paper, we have looked at designing complete, scalable, distributed failure detectors from timing and accuracy
parameters speciﬁed by the distributed application. We
have restricted ourselves to a simple, probabilistically lossy,
network model. Under certain independence assumptions,
we have ﬁrst quantiﬁed the optimal worst-case network load
(messages per second, with a limit on maximal message size)
required by a complete failure detector algorithm in a process group over such a network, derived from applicationspeciﬁed constraints of 1) detection time of a group member
failure by some non-faulty group member, and 2) probability (within the detection time period) of no other nonfaulty member detecting a given non-faulty member as having failed. We have then shown why the popular distributed
heartbeating failure detection schemes inherently do not satisfy this optimal scalability limit.
Finally, we have proposed a randomized failure detector algorithm that imposes an equal expected load on all group
members. This failure detector can be conﬁgured to satisfy the application-speciﬁed requirements of completeness
and accuracy, and speed of failure detection (on average).
Our analysis of the protocol shows that it imposes a worstcase network load that diﬀers from the optimal by a suboptimality factor greater than 1. For very stringent accuracy
requirements (PM (T ) as low as e−30 ), reasonable message
loss probabilities and process failure rates in the network
(up to 15% each), the sub-optimality factor is not as large as
that of traditional distributed heartbeating protocols. Further, this sub-optimality factor does not vary with group
size, when groups are large.
We are currently involved in implementing and testing the
behavior of this protocol in dynamic group membership scenarios. This involves several extensions and optimizations
to the described protocol.

Acknowledgments
We thank all the members of the Océano group for their
feedback. We are also immensely grateful to the anonymous reviewers and Michael Kalantar for their suggestions
towards improving the quality of the paper.

7. REFERENCES
[1] M. K. Aguilera, W. Chen, and S. Toueg. Heartbeat: a
timeout-free failure detector for quiescent reliable
communication. In Proceedings of 11th International
Workshop on Distributed Algorithms (WDAG’97),
pages 126–140, September 1997.
[2] C. Almeida and P. Verissimo. Timing failure detection
and real-time group communication in real-time
systems. In Proceedings of 8th Euromicro Workshop
on Real-Time Systems, June 1996.
[3] K. P. Birman. The process group approach to reliable
distributed computing. Communications of the ACM,
36(12):37–53, December 1993.

[4] R. Bollo, J.-P. L. Narzul, M. Raynal, and F. Tronel.
Probabilistic analysis of a group failure detection
protocol. In Proceedings of 4th International
Workshop on Object-Oriented Real-Time Dependable
Systems, 1998.
[5] T. D. Chandra and S. Toueg. Unreliable failure
detectors for reliable distributed systems. Journal of
the ACM, 43(2):225–267, March 1996.
[6] W. Chen, S. Toueg, and M. K. Aguilera. On the
quality of service of failure detectors. In Proceedings of
30th International Conference on Dependable Systems
and Networks (ICDSN/FTCS-30), June 2000.
[7] S. A. Fakhouri, G. S. Goldszmidt, I. Gupta,
M. Kalantar, and J. A. Pershing. Gulfstream - a
system for dynamic topology management in
multi-domain server farms. Technical Report RC
21954, IBM T.J. Watson Research Center, February
2001.
[8] C. Fetzer and F. Cristian. Fail-awareness in timed
asynchronous systems. In Proceedings of 15th Annual
ACM Symposium on Principles of Distributed
Computing (PODC’96), pages 314–321a, May 1996.
[9] M. J. Fischer, N. A. Lynch, and M. S. Paterson.
Impossibility of distributed Consensus with one faulty
process. Journal of the ACM, 32(2):374–382, April
1985.
[10] I. Gupta, R. van Renesse, and K. P. Birman. A
probabilistically correct leader election protocol for
large groups. In Proceedings of 14th International
Symposium on Distributed Computing (DISC 2000),
LNCS-1914, pages 89–103, October 2000.
[11] J. M. Helary and M. Hurﬁn. Solving Agreement
problems with failure detectors; a survey. Annals of
Telecommunications, 52(9-10):447–464,
September-October 1997.
[12] M. Larrea, A. Fernandez, and S. Arevalo. Optimal
implementation of the weakest failure detector for
solving Consensus. In Proceedings of 19th Annual
ACM-SIGOPS Symposium on Principles of
Distributed Computing (PODC 2000), July 2000.
[13] G. Pﬁster. In search of Clusters, the Ongoing Battle in
Lowly Parallel Computing. Prentice Hall, 1998.
[14] R. van Renesse, Y. Minsky, and M. Hayden. A
gossip-style failure detection service. In Proceedings of
International Conference and Distributed Systems
Platforms and Open Distributed Processing (IFIP),
1998.

