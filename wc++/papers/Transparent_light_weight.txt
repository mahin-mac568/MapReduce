A Transparent Light-Weight Group Service
Luı́s Rodrigues

Katherine Guo

Antonio Sargento

Robbert van Renesse

Brad Glade

ler@inesc.pt

kguo@cs.cornell.edu

amgs@pandora.inesc.pt

rvr@cs.cornell.edu

glade@isis.com

Paulo Verı́ssimo

Kenneth Birman

paulov@inesc.pt

ken@cs.cornell.edu

Abstract
The virtual synchrony model for group communication
has proven to be a powerful paradigm for building distributed applications. Implementations of virtual synchrony
usually require the use of failure detectors and failure recovery protocols. In applications that require the use of a
large number of groups, significant performance gains can
be attained if these groups share the resources required to
provide virtual synchrony. A service that maps user groups
onto instances of a virtually synchronous implementation is
called a Light-Weight Group Service.
This paper proposes a new design for the Light-Weight
Group protocols that enables the usage of this service in
a transparent manner. As a test case, the new design was
implemented in the Horus system, although the underlying
principles can be applied to other architectures as well. The
paper also presents performance results from this implementation.

1

Introduction

Virtually synchronous group communication [1, 2, 13]
has proven to be a powerful paradigm for developing distributed applications. This paradigm allows processes to be
organized in groups within which messages are exchanged
to achieve a common goal. Virtual synchrony ensures that
all processes in the group receive consistent information
about the group membership in the form of views. The
membership of a group may change with time because new
processes may join the group and old processes may fail or
voluntarily leave the group. Virtual synchrony also orders
messages with view changes, and guarantees that all processes that install two consecutive views deliver the same
set of messages between these views.
To provide virtual synchrony, implementations require
 This work was partially supported by the CEC, through ESPRIT/ BRA
Working Group 26 (GODC) and the ARPA/ONR grant N00014-92-J-1866.

the use of failure detectors and the execution of agreement
and ordering protocols. Naturally, these components consume some amount of system resources, such as bandwidth
and processing power. Although the impact of these services
in the overall system performance is usually small, there are
opportunities for optimization when several groups have a
large percentage of common members, because these groups
can share common services. Such opportunities appear in
many applications [6, 11], in particular when object-oriented
programming styles are used [9, 15]. For instance, a parallel
application programmed using an distributed object memory
can create hundreds of groups with similar membership [3].
A technique that allows the previous type of optimization consists of mapping several user level groups onto a
single virtually synchronous group. Since these groups
share common resources, they can be implemented more efficiently than standalone groups and are called Light-Weight
Groups (LWGs ). In contrast, the underlying virtually synchronous group is called in this context a Heavy-Weight
Group (HWG ). A service that maps LWGs onto HWGs is
usually called a Light-Weight Group Service.
The LWG paradigm is being used in several real-world
applications. INFS, a reliable network file system built on
the Isis system, uses this paradigm by associating LWGs with
replicated files. In this system, the replicas for a file change
over time as users change the replication properties of the file
or as access patterns to the file change. The LWG paradigm
lends itself well to this application as the large number of
files amortized over a small set of file replication servers
cause significant sharing of HWGs between LWGs . This setting can be generalized to apply to any application which
multiplexes many object groups over a smaller set of processes or machines. In recognition of this, the paradigm is
supported in the Orbix+Isis product from Isis Distributed
Systems, and Iona Technologies Ltd. Here LWGs play a key
role in reducing the costs of object groups by amortizing
many light-weight membership changes over HWGs .
Light-Weight Group Services have been implemented before in different group based communication systems [6, 11].
Unfortunately, in the previous work, LWGs did not preserve

the exact interface of the underlying virtually synchronous
group, imposing restrictions on group usage. This paper proposes a new design for the LWG protocols that circumvents
such limitations, in particular, it proposes an innovative
dynamic mapping approach that allows the Light-Weight
Group Service to be implemented in a fully transparent manner. As a test case, the new protocols were implemented in
the Horus system [14] (as a new protocol layer) but the
underlying principles can be applied to other architectures
(including the Isis [6] and NAVTECH [16] systems).
The paper is organized as follows. Related work is surveyed in Section 2. The design of the Light-Weight Group
Service is described in Section 3 and the protocols are presented in Section 4. An implementation in Horus is presented in Section 5 and Section 6 concludes the paper.

2

Related work

To our knowledge, Delta-4 [11] was the first system to offer some form of Light-Weight Group Service. The Delta-4
group communication subsystem was structured as a layered architecture, in a fashion similar to that of the ISO
stack. Virtually synchronous support was provided in the
lower layers of the architecture, immediately on top of standard MAC protocols. Several session level groups can be
mapped onto a single MAC level group, but that association was statically defined (such an association is called a
connection in the Delta-4 terminology).
The Isis system has extended this principle, offering a
Light-Weight Group Service that supports dynamic associations between user level groups and core Isis groups [6].
Still, in order to make appropriate mapping decisions, Isis
LWGs require the specification of the target membership of
a user group.
Neither of these approaches is transparent, in the sense
that they do not preserve the original HWG interface. In
both cases, it is necessary to provide additional information,
and it is our belief that this additional information limits the
advantages of LWGs in two ways:

 a powerful feature of virtual synchrony is that it does
not require a-priori knowledge of the group membership; requiring this additional information to implement the LWG protocols reduces the applicability of
the system.
 having a different programming interface, not only
forces existing applications to be changed, but also prevents the LWG protocols from being used as an optional
feature, in a transparent manner.
In this paper, we suggest a new suite of protocols which
offer the LWG abstraction transparently underneath the original HWG interface.

3

Design overview

The main goal of the dynamic LWG Service is to support resource sharing by mapping several LWGs groups with
similar membership onto a single HWG in a way that fully
preserves the original HWG interface. Thus, the mapping
between LWGs and HWGs must be done in a completely automated manner. As a positive side effect of resource sharing, we expect to decrease the latency of group operations
by avoiding redundant start-up procedures.
The LWG Service performs its task by managing a pool
of HWGs and establishing associations between LWGs and
these HWGs . Every time a new LWG is created, the Service
must decide if this LWG should be associated with one of
the already created HWGs (if any), or if a new HWG should
be added to the pool. Whatever decision is made, the new
LWG will be associated with some HWG and will share that
HWG with other LWGs . Since the design imposes no restriction in the way the membership of LWGs changes in time,
mappings that are appropriate at one point may become inefficient as the system evolves. To compensate these changes,
the LWG Service allows mappings to be dynamically redefined. In such cases we say that a LWG is switched from one
HWG to another. If at some point in time a given HWG seems
to become unsuitable for establishing further mappings, it
is released from the pool. Thus, the pool of HWGs managed
by the Service expands and shrinks in time, not only due
to the creation of new LWGs , but also due to changes in
membership in these groups.
The LWG service then has three main tasks: (i) preserves the virtually synchronous interface of the HWGs to the
LWGs users; (ii) defines the mapping and switching policies;
and (iii) invokes a switching protocol, which is a protocol
that allows the association between a LWG and a HWG to be
changed at run time. In this paper we present the protocols
that allow us to achieve the first and third of these tasks. The
first task is a critical point in the overall design as, if no performance advantages can be obtained by mapping several
LWGs onto a single HWG , the implementation of mapping
and switching strategies becomes pointless. In this paper
we describe the protocol for switching between groups but
not the decision process that leads to the invocation of this
protocol (mapping and switching heuristics are discussed in
another report [12]).

4

Protocols

This section describes the protocols that implement the
Light-Weight Group Service. These protocols perform several tasks required to offer virtual synchrony: join a group,
leave a group, and multicast messages in a group. For
self-containment, the switching protocol is also presented.

The section starts by presenting the assumptions about the
underlying (Heavy-Weight) virtually synchronous service.

4.1

Assumptions

The Light-Weight Group Service described in this paper
was designed to run on any of a set of group communication architectures. Particularly, the service was designed
with the Isis, Horus and NAVTECH systems in mind. All
these systems provide a virtually synchronous communication service.
4.1.1 Virtual synchrony. Informally, virtual synchrony
provides group membership information to each process in
the form of views and guarantees that all processes that install two consecutive views deliver the same set of messages
between these views. More formally, virtually synchronous
multicast communication can be defined as follows [13]:
vs-multicast: Consider a set of processes g, a view V i (g),
and a message m multicast to the members of group
V i (g). The multicast of message m is called a vsmulticast iff the following property is satisfied. If
9p 2 V i (g) which has delivered m in view V i (g)
and has installed view V i+1 (g), then every process
q 2 V i (g) which has installed V i+1 (g) has delivered
m before installing V i+1(g). The system is virtually
synchronous iff every multicast is a vs-multicast.
This definition imposes a total order between view
changes and multicasts, but does not enforce any ordering
between messages delivered in the same view. However, in
this paper, we further assume that the virtually synchronous
layer delivers messages according causal precedence (and
that this guarantee is preserved across different groups).
The implementation of virtual synchrony requires the use
of a failure detector plus the execution of some form of flush
protocol to ensure that all messages delivered to some processes in a given view are delivered to all correct processes
in that view before a new view is installed. To guarantee
the termination of the flush protocol, the traffic may be temporarily stopped during the protocol execution. This may
lead to a short system slow down during the execution of the
view change protocol, but simplifies application design (for
example, a process that multicasts a message can deliver
it locally immediately without any further computation or
bookkeeping). However, protocols exist that allow the continuation of the message flow during view changes [5]. It
is also possible to implement a membership service that addresses explicitly the problem of network partitions [4, 10].
The implementation of the LWG service on top of such membership services is outside the scope of this paper.

Downcalls
Name
Join
Leave
Send
HoldOk

Parameters
GroupId gid, Pid pid
GroupId gid, Pid pid
GroupId gid, BitArray data
GroupId gid

Name
View
Data
Hold

Parameters
GroupId gid, PidList view
GroupId gid, Pid src, BitArray data
GroupId gid

Upcalls

Table 1. VS interface primitives
4.1.2 Interface. A typical interface of a virtually synchronous layer contains the following primitives, as listed
in Table 1 (we denote the downcalls with the “.req” suffix and the upcalls with the “.int” suffix): Join.req,
is invoked by a member that wants to join a group;
Leave.req, invoked by a member that wishes to leave a
group; Send.req, is used to send a virtually synchronous
multicast; View.int, installs a new view; Data.int,
indicates the delivery of a multicast; Hold.int, indicates
that the traffic must be stopped temporarily (usually, when
a view change in the virtually synchronous layer is in process); and HoldOk.req, is used to confirm the Hold.int
indication. Hold.int and HoldOk.req may be hidden
from the user at upper layers.

User layer

LWG

LWG

Lwg.View.int

LWG

Lwg.Data.int

LWG

LWG

Lwg.Hold.int

Lwg Interface
Lwg.Leave.req

Lwg.Join.req

Lwg.Send.req

Lwg.HoldOk.req

Dynamic Mapping Protocols

Hwg.View.int

Hwg.Data.int

Hwg.Hold.int

Hwg Interface
Hwg.Leave.req

Hwg.Join.req

HWG

Hwg.Send.req

Hwg.HoldOk.req

HWG

HWG Layer

Figure 1. Light-Weight Group service interface

The main goal of our design is to build a service that
allows several user groups to share the same virtually synchronous group in a transparent manner. Thus, the LightWeight Group Service should export the same interface as
the virtual synchrony service, as illustrated in Figure 1.
The behavior of the interface is described by the state
machine illustrated in Figure 2. When the interface is not
active, it is in the Idle state. As a response to a Join.req,
it leaves this state to the Joining state where it remains until
a view that contains the local process is received. From
then on, the interface is said to be in Running state, and
can accept Send.req requests as well as Data.int interrupts. When there is the need to install a new view, the
user is requested to temporarily stop sending new messages
through the Hold.int. The interface remains in the Holding state until the user acknowledges this request through
the HoldOk.req. The interface is then in the WaitView
state, where messages from the current view can be delivered but no new messages can be sent. When a new view is
received (View.int) the interface returns to the Running
state. Finally, if the application wants to leave the group it
issues a Leave.req and the interface goes to the Leaving
state, where a view excluding the local process from the
group is awaited before returning to the Idle state.
Send.req
Data.int

Join.req

Joining
Idle

Idle

Send.req
Data.int

Holding
View.int
View.int
Hold.int
Data.int

Leaving
Idle

HoldOk.req

Running
Data.int
Leave.req

View.int

WaitView

Figure 2. VS interface state machine
It should be noted that the details of the actual interface
of each of the target architectures may differ. In particular,
the details of the interface for the case of the Horus system
will be presented in Section 5.
4.1.3 Storing mappings. The implementation of the
Light-Weight Group Service requires mappings between
LWGs and HWGs to be stored in a way that can be accessed
by every process. Typically, when Join.req is issued
at some process, that process has to find out if the associated LWG is already mapped onto some HWG . In this
paper we assume that mappings are stored in some external
Name Service. The name service exports three primitives,
as illustrated in Table 2, namely: ns.set, which establishes a mapping between a LWG and a HWG ; ns.read,
which returns the current mapping for a given LWG ; and
ns.testset, which returns the current mapping for a

Name

Parameters

Returns

ns.set
ns.read
ns.testset

LwgId lwg, HwgId hwg
LwgId lwg
LwgId lwg, HwgId hwg

none
HwgId hwg
HwgId hwg

Table 2. Name service interface primitives
given LWG or, if no such mapping exits, established a new
mapping to the HWG specified. This last primitive is offered
both to minimize the number of accesses to the name service and to prevent race conditions among processes that
concurrently try to establish new mappings for the same
LWG .
Note that, for availability, the name service may be replicated. A possible implementation would replicate the name
service at every process, making updates expensive but read
operations purely local.

4.2

Variables

The protocols use the following variables for each LWG :
lwgId, the identifier of the LWG ; currentHwg, the identifier of the HWG on which the LWG is currently mapped;
nextHwg, the identifier of the HWG where the LWG is
going to be mapped in the future (usually the same as
currentHwg, except when a switch is being executed);
currentView, the current group view of the LWG ;
joiningList, a list of processes that want to join the
LWG ; leavingList, a list of processes that want to leave
the LWG ; state, the current state of the protocol, which is
one of the states showed in Figure 2; nacks, some protocols require an acknowledgment to be collected from every
group member (the number of acknowledgments received
is collected in this variable); doFlush, a boolean variable which is set whenever a flush needs to be performed;
coordinator, a boolean flag which is set to true when
the local process is the oldest member of the LWG .
Additionally, for each HWG , the following variables
are also used: hwgId, the identifier of the HWG ;
currentView, the current group view of the HWG ;
mappedLwgs, a list of LWGs mapped onto this HWG ;
nHoldOk, the number of HoldOk.req acknowledgments
received. Sometimes, in order to flush the HWG , the traffic
must be stopped at all mapped LWGs , nHoldOk is used
to keep track of how many LWGs have acknowledged an
lwg.Hold.int.

4.3

The flush protocol

The core of the Light-Weight Group implementation is
the flush protocol, which is responsible for installing a new
view. The protocol is illustrated in Figure 3. The protocol
is initiated by the coordinator that multicasts a FLUSH mes-

300
301
302
303
305
306
307
308
309
310
311
312
313
314
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339

=

when lwg.doFlush and lwg.coordinator and lwg.state Running do
lwg.doFlush : FALSE; lwg.nacks : 0;
hwg.Send.req ( lwg.currentHwg, hFLUSH, lwg.lwgIdi);
od

=

=

when hwg.Data.int (hFLUSH, lwgIdi) received do
lwg.Hold.int ( lwg.lwgId ); lwg.state : Holding;
od

=

when lwg.HoldOk.req ( lwgid ) do
lwg.state : WaitView;
hwg.Send.req ( lwg.currentHwg, hFLUSH OK, lwg.lwgIdi )
od

=

when hwg.Data.int (hFLUSH OK, lwgIdi) received do
lwg.nacks : lwg.nacks + 1;
od

=

=
=
=

when lwg.nacks # lwg.curentView and lwg.coordinator do
newView : lwg.currentView \ lwg.joiningList - lwg.leavingList;
viewMess : hVIEW, lwg.lwgId, lwg.nextHwg, newViewi ;
hwg.Send.req ( lwg.currentHwg, viewMess );
od
when hwg.Data.int (hVIEW, lwgId, nextHwg, newViewi) received do
if local process in newView then
lwg.currentView : newView;
lwg.joiningList : lwg.joiningList - newView;
lwg.leavingList : lwg.leavingList \ newView;
lwg.currentHwg : nextHwg;
if lwg.coordinator then
ns.set ( lwg.lwgId, lwg.currentHwg ); fi
lwg.state : Running;
else
lwg.state : Idle;
fi
lwg.View.int ( lwg.currentView );
od

=
=
=
=

=
=

4.4

The create/join protocol

The create/join procedure consists of two main steps, as
illustrated in Figure 4. In the first step, a map is established
between the LWG and some HWG (l. 401). To minimize
accesses to the name service, the joining process proposes
a mapping based on its own local HWGs according to the
mapping heuristics [12]. Then, in a single access to the
name service it commits this mapping or, in the case where
the LWG is already mapped onto some other HWG, obtains
the existing mapping (l. 404). Additionally, if the process is
not a member of the selected HWG , it joins the HWG before
executing the second step (l. 405).
400 when lwg.Join.req ( lwgId, processId ) do
401
// first step
402
lwg.lwgId := lwgId; lwg.state := Idle;
403
lwg.currentHwg := proposeLocalMapping ();
404
lwg.currentHwg := ns.testset ( lwgId, lwg.currentHwg );
405
if local process not member of hwgId then
406
hwg.Join.req ( lwg.currentHwg );
407
wait hwg.View.int (lwg.currentHwg);
408
fi
409
localMap ( lwg.lwgid, lwg.currentHwg );
410
// second step
411 lwg.state := Joining;
412
hwg.Send.req (lwg.currentHwg, hJOIN, lwg.lwgId, processIdi );
413 od
414 when hwg.Data.int (hJOIN, lwgId, processIdi) received do
415
lwg.joiningList := lwg.joiningList  processId;
416
lwg.doFlush := TRUE;
417 od

Figure 3. Flush protocol

Figure 4. The create/join protocol

sage when the doFlush flag is set (we will later show the
scenarios that trigger this condition). When the FLUSH
is received, the application is requested to stop sending
through the Hold.int interrupt (l. 307). When the corresponding HoldOk.req is received from the application,
the LWG member acknowledges the FLUSH message with
a FLUSH OK (l. 312). The protocol is terminated by
the coordinator that sends a VIEW message as soon as a
FLUSH OK is received from every member (l. 320). When
the VIEW message is received, the traffic is resumed by
delivering the new view through the lwg.View.int interrupt (l. 326). In addition to the new membership of the
group, the VIEW messages disseminates the identity of the
HWG that should be used during the next view (l. 331).
Thus, the flush protocol is used both to change the group
membership and to execute the switch protocol. If a member process fails or becomes unreachable while executing
the flush protocol, another round of the flush protocol starts
immediately, collecting FLUSH OK replies from currently
available members. Therefore the flush protocol can not
block.

The second step consists of sending a JOIN message to
all members of the HWG (l. 412). When the JOIN message
is received, the identifier of the joining process is added
to the joiningList and doFlush flag is activated (l.
414). The coordinator of the LWG will then trigger a flush
protocol which, in turn, will install a new view.

4.5

The leave protocol

The leave procedure in Figure 5 is similar to the joining
protocol. The process simply sends a LEAVE message
to all members of the HWG (l. 503). When the LEAVE
message is received, the identifier of the process is added to
the leavingList and the doFlush flag is activated (l.
506). The coordinator of the LWG will then trigger a flush
protocol which, in turn, will install a new view.

4.6

The message passing protocol

The principle of the message passing protocol is very simple. The LWG service simply encapsulates the LWG message

500
501
503
504
505
506
507
508
509

when lwg.Leave.req ( lwgId, processId ) do
lwg.state := Leaving;
hwg.Send.req ( lwg.CurrentHwg, hLEAVE, lwgId, processIdi );
od
when hwg.Data.int (hLEAVE, lwgId, processIdi) received do
lwg.leavingList := lwg.leavingList  processId;
lwg.doFlush := TRUE; // will trigger flush
od

Figure 5. The leave protocol
in a dedicated hDATA, lwgid, datai message which is multicast on the HWG . On the recipient side, when such message
is received the lwgid part is examined and the data part is
forwarded to the specified LWG .
A message multicast on a HWG could be performed using
two main approaches. In the first approach, the message is
multicast to all members of the HWG and then each site that
is not a member of the concerned LWG discards the message.
This has two disadvantages:

 it makes the multicast more expensive, since more destination sites are used than those strictly needed;
 it consumes resources to handle the received messages
at those sites.
The other approach consists of using some form of selective address mechanism, which allows to multicast a message in a HWG just to a subset of all the members of the HWG .
An approach similar to this was used in the Delta-4 [11] and
Isis lightweight group mechanisms [6].

4.7

The switch protocol

While this paper does not focus on policies or heuristics
for deciding when to change a LWG to HWG mapping, the
algorithm for switching between HWGs is briefly described.
Assume that a given LWG , lwgId, needs to be switched
from one HWG , hwgFrom, to another HWG , hwgTo. The
switch protocol is initiated by some process member of
lwgId. In order to inform other members of lwgId of the
start of the switching procedure, it multicasts an hOPEN,
lwgId, hwgToi message on hwgFrom (l. 601). When this
message is received, all members of lwgId check if they
are already members of hwgTo and, in case they are not,
join this group (l. 603).
When a member of the LWG detects that all members have
joined hwgTo, it sets the variable nextHwg and activates
the doFlush flag (l. 608). As in the previous cases, this
will trigger the execution of the flush protocol which will
install a new view and commit the new mapping. The switch
protocol is presented in Figure 6.

600 when lwgId needs to be switched to hwgTo do
601
hwg.Send.req ( lwg.currentHwg, hOPEN, lwgId, hwgToi );
602
when hwg.Data.int (hOPEN, lwgId, hwgToi) received do
603
604
// OPEN is received through hwgFrom
605
if I am not member of hwgTo then
606
hwg.Join.req ( hwgTo ); fi
607
od
608
when lwg.currentView  hwgTo.currentView do
609
lwg.nextHwg := hwgTo; lwg.doFlush := TRUE;
610
od
611 od

Figure 6. The switch protocol

4.8

The failure handling protocol

The basic failure handling protocol is quite simple because most of the complexity is handled by the virtually
synchronous service. Whenever a failure is detected by a
HWG , a Hold.int is generated in order to stop the traffic flow (l. 700). This interrupt must be multiplexed to
all LWGs mapped onto that HWG (see Figure 7). The LightWeight Group Service waits for an acknowledgment from
every LWG (in-transit messages can still be sent or received)
and then acknowledges the Hold.int interrupt (l. 706).
Finally, when a new view is installed in the HWG , the failed
processes are removed from the views of all mapped LWGs (l.
713).

700
701
702
703
704
705
706
707
708
709
710
711
712
713
714
715
716
717
718
719
720
721
722
723
724

when hwg.Hold.int (hwg) do
forall lwg in hwg.mappedLwg
lwg.Hold.int (lwg);
endfor
od
when lwg.HoldOk.req (lwg) do
hwg.nHoldOk := hwg.nHoldOk +1;
if hwg.nHoldOk # hwg.mappedLwg then
hwg.HoldOk.req (hwg.hwgId);
fi
od

=

when hwg.View.int (hwgId, newview) do
hwg.currentView := newview;
forall lwg in hwg.mappedLwg do
lwg.currentView : lwg.currentView \ newView;
lwg.joiningList : lwg.joiningList \ newView;
lwg.leavingList : lwg.leavingList \ newView;
lwg.View.int (lwg.lwgId, lwg.curentView);
if local process oldest in lwg.currentView then
lwg.coordinator := TRUE;
fi
endfor
od

=
=
=

Figure 7. Failure handling

4.9

Synchronization with the name server

When a switch occurs, the name service is informed of the
new mapping so that further joins are directed to the appropriate HWG . A problem of using an external name service
to keep information about the mapping between LWGs and
HWGs , is that it is difficult to guarantee that processes always
read up-to-date information. To avoid expensive synchronization procedures, we allow processes to read outdated
information (for instance, when a read to the name service is
executed concurrently with the execution of the switch protocol). To compensate for this, all members of a HWG keep
information about the new mappings of previously mapped
LWGs . This information is used like a forward-pointer, to
redirect a process that is using outdated mapping information. Forward-pointers are discarded based on the passage
of time. Thus, we assume that when a process gets a mapping from the name service, this information is valid just for
some reasonable period of time (in some sense, it works as
a lease [7]).

4.10 Interleaving of protocols
The final protocols are slightly more complex than the
ones presented in this paper due to the possible interleaving
of the failure handling protocol with the remaining protocols. The complete protocols are not presented here due to
lack of space but can be found in the full report [12].

5
5.1

An implementation in Horus
Horus overview

Horus is a group communication system which offers
great flexibility in the properties provided by protocols. It
uses virtually synchronous protocols to support dynamic
group membership, message ordering, synchronization and
failure handling.
In the Horus architecture, protocols are constructed dynamically by stacking microprotocols, which support a common interface. Each microprotocol offers a small integral
set of communication properties, and is implemented as a
layer in Horus. Each layer has a set of entry points for
downcall and upcall procedures denoted with the “.req”
and “.int” suffixes respectively.
Horus provides a large set of microprotocols. The following are related to our design of the Light-Weight Group
Service. The COM layer provides the Horus interface
over other low-level communication interfaces (including
IP, UDP, ATM, the x-kernel and a network simulator). The
NAK layer provides reliable FIFO unicast and multicast.
The FRAG layer implements fragmentation and reassembly

of messages. The MBRSHIP layer guarantees virtual synchrony. The CAUSAL and TOTAL layers offer causally
and totally ordered message delivery respectively.

5.2

Horus virtual synchrony protocols

The MBRSHIP layer in Horus implements virtually synchronous membership and message atomicity. During message transmission, members of the group are constantly collecting stability information of all the messages they have
sent or received. A message is stable if it has been received
by every member of the group. Virtual synchrony is ensured by a flush protocol that is conceptually similar to that
presented in this paper. However, the implementation of the
flush protocols in Horus, both in the MBRSHIP layer and in
the LWG layer, uses a coordinator based approach to reduce
the number of multicast messages exchanged.
In the MBRSHIP layer, the oldest member in a view
is designated as the coordinator. During a membership
change, the coordinator decides which members are correct and should be included in the next view. It broadcasts
a FLUSH message to the surviving members, requesting
them to stop sending messages and to ignore messages from
incorrect members. Upon receipt of a FLUSH, a member
forwards to the coordinator its unstable messages followed
by a FLUSH OK message (these messages are point-topoint). When the coordinator has received a FLUSH OK
message from all correct processes in the current view, it
rebroadcasts those unstable messages. Upon receiving rebroadcast messages, the members ignore those it has already
delivered. The flush is completed after all the messages have
stabilized. At this point a new view may be installed.
In our implementation, the LWG layer is put on top of
the “MBRSHIP:FRAG:NAK:COM” stack. The LWG flush
protocol is implemented using a coordinator based solution
where the FLUSH OK and VIEW messages carry the causal
dependencies required to automatically flush data messages.
Currently, all LWG messages are sent in multicast to all members of the HWG .

5.3

Performance

We conducted the performance tests for LWG in Horus
on a system of SUN Sparc10 workstations running SunOS
4.1.3, connected by a loaded 10M bps Ethernet. The lowlevel protocol we used is UDP/IP with the Deering multicast
extension. We tested n identical four-member groups using
four machines with one process per machine.
We conducted three different types of tests to measure the
impact of LWG Service on: (i) group membership operation,
(ii) failure handling and (iii) data transfer. In these tests, every group member has the stack “LWG:MBRSHIP:FRAG:NAK:COM” underneath it. To evaluate the effectiveness

of our approach, the exactly same tests were run without
the LWG layer. In the rest of the section, all the flush time
measurements are taken at the coordinator. When a member
joins, the flush time is measured between a Join.req and
a View.int. When a member leaves, the flush time is
measured between the receipt of the LEAVE message and
the following View.int.

HWG −−

2500

JoinTimeLWG (p n) = FlushHWG (p) + FlushLWG (p)  n

2000
LWG −

join time (msec)

JoinTimeHWG (p n) = FlushHWG (p)  n
When the LWG layer is used, this time can be expressed
as (where FlushLWG (p) is the amount of time for each
LWG flush):

Joining n groups as the third member.
3000

1500

1000

500

0
0

of executions of the flush protocol. Without the LWG layer,
the time for joining n groups of p ; 1 members, denoted
JoinTimeHWG (p n), can be approximately1 expressed as
follows (where FlushHWG (p) is the amount of time for each
HWG flush when the resulting group size is p):

20

40

60

80
100
120
140
total # of groups n
Joining n groups as the fourth member.

160

180

200

3500

3000

2500

In this case, when the process joins the first of the n
LWGs , it joins the underlying HWG first. As a result, the
first join involves a HWG flush and a LWG flush.
In the Horus implementation, the flush process is identical for both HWGs and LWGs . In either case, the coordinator waits until it has collected FLUSH OK messages
from all other members and, as soon as the flush is done,
installs a new view. The difference between FlushHWG (p)
and FlushLWG (p) solely comes from the configuration of
resources in the underlying layers. In the HWG approach,
the MBRSHIP, FRAG, NAK and COM layers need to be
reconfigured every time a new view is installed (this is performed by installing the new view in all these layers). In
the LWG approach, these resources are shared and need to
be reconfigured only once.

HWG −−

Leaving one group only.

join time (msec)

30
2000

LWG −

25
1500

leave time (msec)

1000

500

0
0

20

40

60

80
100
120
total # of groups n

140

160

180

200

Figure 8. Join n groups

5.3.1 Membership operations. To evaluate the effect
of LWGs on membership operations, we measured the total
flush time at the coordinator when another process joins,
one by one, n groups with the same membership. We measured the flush time between the time when the coordinator receives the first Join.req and the last View.int.
Figure 8 shows the flush time when a process joins as
the third and fourth member. The time required to perform membership operations is a function of the number

20

HWG −−

15

LWG −

10

5

0
0

20

40

60

80
100
120
total # of groups n

140

160

180

200

Figure 9. Leave one group
The difference between FlushHWG (p) and FlushLWG (p)
is shown in Figure 9 where the flush time is measured when
a non-coordinator leaves one of the n four-member groups.
1 The measured time is smaller since there is some degree of parallelism
among concurrent flushes.

Total crash handling time for n LWGs
100
90
80

recovery time (msec)

The improvement of LWGs over HWGs in this case is not
impressive. Still, it provides some amount of optimization
whose benefit becomes non-neglegible for large number of
groups. When several joins are requested in bursts, the performance can be further improved by piggybacking several
independent joins and executing them in a single operation.
We are planning to modify the Horus interface to allow
a process to join n LWGs at once, and then build a “joinpiggyback” layer that can be inserted at any point in the
stack.

70
60
50
40
30

5.3.2 Failure recovery. To evaluate the effect of LWGs on
failure recovery, we conducted the following test: a given
process, member of n identical four-member groups, crashes
and forces these n groups to reconfigure. The recovery
time, measured between the detection of the failure and the
installation of a new view2 is presented in Figure 10.

20
10
0
0

20

40

60

80
100
120
total # of groups n

140

160

180

200

Figure 11. Recovery from crashes (LWG )
One process crashes −− leaving n groups.
3000

3

2500

2.5

2000

one−way latency (msec)

recovery time (msec)

HWG −−

LWG −
1500

1000

2

HWG −−

1.5

LWG −

1

500

0.5

0
0

20

40

60

80
100
120
total # of groups n

140

160

180

200

0
0

20

40

60

80
100
120
total # of groups n

140

160

180

200

Figure 10. Recovery from crashes (comparative)

Figure 12. Data transfer

Again, the installation of a new view is preceded by a
flush operation. Since a failure is notified at each of the n
groups, each group starts its own flush. In the HWG test,
n HWG flushes need to be run in parallel. On the other
hand, the LWG layer multiplexes the flushes of all LWGs in a
single flush of the underlying shared HWG . Thus, the total
recovery time for HWGs shows a more than linear increase
as n increases, whereas for LWGs , the total recovery time
increases linearly with a very flat slop. This small linear
increase is due to the fact that, in any case, all LWGs need to
be notified of the group membership change. For readability,
a scaled illustration of this relatively flat slop is shown in
Figure 11.

5.3.3 Data transfer. To evaluate the impact of LWG on
data transfer, we measured one-way latency when one member is multicasting 10-Byte messages in one of the n fourmember groups. Figure 12 shows that up to n = 50, the
one-way latency of the HWG test is slightly better than that
of the LWG test, with the difference being 20 microseconds.
After n = 50, The LWG figure stays constant at 1.25 milliseconds, while the HWG figure increases dramatically from
1.28 to 2.90 milliseconds as n increases from 50 to 200.
It is interesting to discuss the causes for these behaviors.
In order to offer timely failure detection and reliable FIFO
communication, the NAK layer in Horus has each group
member multicast one “status” report background message
every 2 seconds. Every member therefore receives one
“status” report every 2 seconds. When there are n HWGs on
each process, a total of n=2 background messages need to be

2 In

Horus, failure detection is performed at the lower layers [8].

handled every second. Experiments show that the network
bandwidth is more than enough to handle n=2 IP multicasts
per second of small background messages even when n =
200. The bottleneck is the receiver processing speed [8].
As n increases, the process is not fast enough to handle
all the incoming messages, therefore, it drops them from
the input buffer. The resulting requests for retransmissions
and retransmissions themselves add even more load to the
system. This snowball effect causes the flush time and data
transfer latency for n groups to increase dramatically with
n.
These results show that the resource sharing promoted
by the LWG approach offers clear performance advantages.
It is interesting to observe that the significant improvements
in failure recovery are not achieved at the cost of degrading other services. On the contrary, the performance of
data transfer and join operations is also improved for large
number of groups.

6

Conclusions and future work

In this paper we have presented a technique that promotes resource sharing among groups that have the same
or similar membership. This is achieved by executing, in
a fully transparent manner, a set of inexpensive protocols
on top of a virtually synchronous layer. An implementation of these protocols in the Horus system has shown that
this approach offers clear performance advantages. The experiments were done in a environment where the mapping
between light-weight groups and heavy-weight groups remains constant over significant periods of operation. We
are currently experimenting with switching heuristics (that
dynamically modify this mapping) to extend these results to
less stable group patterns.

[5]

[6]

[7]

[8]

[9]

[10]

[11]

[12]

[13]

[14]

Acknowledgements
[15]

The authors are grateful to the anonymous referees for
their comments on an earlier version of this paper.
[16]

References
[1] K. Birman and T. Joseph. Exploiting replication in distributed systems. In S. Mullender, editor, Distributed Systems, pages 319–366. ACM Press Frontier Series, 1989.
[2] K. Birman and R. van Renesse, editors. Reliable Distributed
Computing With the ISIS Toolkit. Number ISBN 0-81865342-6. IEEE CS Press, Mar. 1994.
[3] M. Castro and N. Neves. Group communication support for
parallel applications in a cluster of workstations. Technical
report, INESC-IST, June 1994.
[4] D. Dolev, D. Malki, and R. Strong. An asynchronous membership protocol that tolerates partitions. Technical Re-

port Research Report, The Hebrew University of Jerusalem,
1993.
R. Friedman and R. van Renesse. Strong and weak virtual
synchrony in horus. In 15th Symposium on Reliable Distributed Systems, Oct. 1996.
B. Glade, K. Birman, R. Cooper, and R. van Renesse. Lightweight process groups in the isis system. Distributed System
Engineering, (1):29–36, 1993.
G. Gray and D. Cheriton. Leases: An efficient fault-tolerant
mechanism for distributed file cache consistency. In Proceedings of the Twelfth ACM Symposium on Operating Systems Principles, pages 202–210, Litchfield Park, Arizona,
Dec. 1989.
K. Guo, W. Vogels, and R. van Renesse. Structured virtual
synchrony: Exploring the bounds of virtually synchronous
group communication. In Proceedings of the 7th ACM
SIGOPS European Workshop, Sept. 1996.
O. Hagsand, H. Herzog, K. Birman, and R. Cooper. Objectoriented reliable distributed programming. In Proceedings
of 2nd International Workshop on Object-Orientation in Operating Systems, 1992.
L. Moser, Y. Amir, P. Melliar-Smith, and D. Agarwal. Extended virtual synchrony. In Proceedingsof the 14th International Conference on Distributed Computing Systems, pages
56–65, Poznan, Poland, June 1994.
D. Powell, editor. Delta-4 - A Generic Architecture for Dependable Distributed Computing. ESPRIT Research Reports. Springer Verlag, Nov. 1991.
L. Rodrigues, K. Guo, A. Sargento, R. van Renesse,
B. Glade, P. Verı́ssimo, and K. Birman. A dynamic light
weight group service. Technical report, INESC/Cornell
Univ., Mar. 1996.
A. Schiper and A. Ricciardi. Virtually-synchronous communication based on a weak failure suspector. In Digest of Papers, The 23th International Symposium on Fault-Tolerant
Computing, pages 534–543, Toulouse, France, June 1993.
R. van Renesse, K. Birman, R. Cooper, B. Glade, and
P. Stephenson. Reliable multicast between microkernels.
In Proceedings of the USENIX Workshop on Micro-Kernels
and Other Architectures, pages 269–283, Seattle, Washington, Apr. 1992.
R. van Renesse, K. Birman, and S. Maffeis. Horus, a flexible
group communication system. Communications of the ACM,
39(4):76–83, Apr. 1996.
P. Verı́ssimo and L. Rodrigues. The NavTech large-scale distributed computing platform. Technical report, FCUL/IST.
(in preparation).

